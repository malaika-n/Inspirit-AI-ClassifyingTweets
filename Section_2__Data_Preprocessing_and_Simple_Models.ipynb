{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malaika-n/Inspirit-AI-ClassifyingTweets/blob/main/Section_2__Data_Preprocessing_and_Simple_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3knkHZcWKp2k"
      },
      "source": [
        "# Project: Disaster Relief\n",
        "#Section 2: Data Reprocessing and Simple Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5gBwlTa5EUR"
      },
      "source": [
        "Today we will be building some more sophisticated models called One-hot-encoding, CountVectorizor, and Bag of Words. You can review the slides to remember how these models work. But first we will need to clean our data a little bit, an important step in any AI Pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpVg1HHxHoV9"
      },
      "source": [
        "#@title Load your dataset { display-mode: \"form\" }\n",
        "# Run this every time you open the spreadsheet\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from collections import Counter\n",
        "from importlib.machinery import SourceFileLoader\n",
        "import numpy as np\n",
        "from os.path import join\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords' ,quiet=True)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn import metrics\n",
        "import gdown\n",
        "\n",
        "\n",
        "gdown.download('https://drive.google.com/uc?id=1umFXM7SvdBvTlHW0r0CXDcxNqL73jU8Z', 'disaster_data.csv', True)\n",
        "\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYFUn48ZeX5C"
      },
      "source": [
        "# Introduction - What is A Model?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHy-dPU0c7bD"
      },
      "source": [
        "Up until now, we have looked at disaster-related tweets, made a rule based classifier, and evaluated it. Today, we shall clean the data, stem it, and learn about One-Hot Encoding, CountVectorizer, and Logistic Regression.\n",
        "\n",
        "Before, we do all this, we must understand what a model really is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyDHWJg1eZdG"
      },
      "source": [
        "What is a model? A model is something that we will make to predict the category of a given tweet. It is a numerical understanding of the data such that given a new data point, we can figure out how it links to the previous data. This is a crude definition, which you will understand more through this project.\n",
        "\n",
        "The models we have used or will use in this project are:\n",
        "\n",
        "1. Rule Based Classifier (You specify rules based on which the model gives the output - the category of the tweet)\n",
        "2. CountVectorizer + Logistic Regression: A model based on counting the number of occurances of a word and applying regression on it to predict the category of a new tweet\n",
        "3. Word2Vec + Logistic Regression: A model which applies the idea that words that occur in similar contexts tend to be close in a sentence, and uses this to predict the category of the tweet (It is quite difficult to understand this, so don't worry if you don't get it in the first go! More explanation on this later.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCUv39e9UbZS"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Before we can jump into building a model, we must clean the data a little bit!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xsl3tDctOMo"
      },
      "source": [
        "# Load the data.\n",
        "disaster_tweets = pd.read_csv('disaster_data.csv',encoding =\"ISO-8859-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ6yDnM_Lyoe"
      },
      "source": [
        "disaster_tweets.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6YI_yKLtOMu"
      },
      "source": [
        "**Discussion Exercise**: Consider the tweet *I really need food.... I am very hungry. The hunger is unbearable. #pleasehelp*\n",
        "\n",
        "1. Are all words in the tweet equally informative?\n",
        "2. Are there any words in this sentence that mean the same thing, but are technically distinct words?\n",
        "3. Are there any unecessary words or symbols that we could remove from the tweet before building a model?\n",
        "\n",
        "We are going to play with three pre-processing steps to address these two questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA464Y6cLTvY"
      },
      "source": [
        "###Removal of non alphabetic characters\n",
        "\n",
        "In tweet classification we use words as the features, so it's important to remove unwanted characters such as numbers and punctuation marks as they dont provide us with any valuable information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipcc-gW4pV_H"
      },
      "source": [
        "#Read the tweet data and convert it to lowercase\n",
        "tweets = disaster_tweets['text'].str.lower()\n",
        "tweets = tweets.apply(lambda x: re.sub(r'[^a-zA-Z0-9]+', ' ',x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ4U8NqwLqkC"
      },
      "source": [
        "#Extract the labels from the csv\n",
        "tweet_labels = disaster_tweets['category']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZegZdHEtOMu"
      },
      "source": [
        "### Tokenizing\n",
        "First we need to split a sentence into individual words, or *tokens*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBHJd1zzbP5y"
      },
      "source": [
        "###**Discussion Exercise**:  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhQawpZhZh0i"
      },
      "source": [
        "What would be the token list be of the following sentence be? \"*AI is so fun! I love to learn about NLP and machine learning!*\"\"\n",
        "\n",
        "Discuss your answer with your group, then write it down in your worksheet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9Kp7TmUP4jV",
        "cellView": "form"
      },
      "source": [
        "#@title Tokenizing\n",
        "tweet = \"AI is so fun! I love to learn about NLP and machine learning!nter your own tweet here\" #@param {type:'string'}\n",
        "for i in word_tokenize(tweet):\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOdgIHG1XyTc"
      },
      "source": [
        "## Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxy-DT6VX1ow"
      },
      "source": [
        "Remember that the goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
        "\n",
        "A difference between stemming and lemmatization is that stemming looks at the current word only, while lemmatization also takes the context into consideration. Either way, this pre-processing step could be somewhat tedious. Luckily, the powerful `nltk` provides tools for both.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqiJUBS_tONG"
      },
      "source": [
        "\n",
        "### Exercise: Stemming using the Porter stemmer\n",
        "*Porter's algorithm*, developed in the 1980s, is one of the most commonly used stemmers.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOZgX-PUX6c8"
      },
      "source": [
        "\n",
        "Try and find a word that Porter's stemming doesn't work well on! (Hint: Try some plurals of words that end in -e)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZJvHmIttONI"
      },
      "source": [
        "#@title Stem words { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "stemmer = PorterStemmer()\n",
        "word = \"\" #@param {type:\"string\"}\n",
        "print(stemmer.stem(word))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDL4h--FtONM"
      },
      "source": [
        "### Lemmatizer\n",
        "\n",
        "You can add more words to `plurals` and see what the stemming results look like.  \n",
        "You may find that the results may look a bit mechanical. This is because the Porter's algorithm is essentially a sequential application of a set of rules. To get better looking results, let's try out a lemmatizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VidwNdk0tONO"
      },
      "source": [
        "#@title Lemmatize Words { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "# Get the lemmatizer\n",
        "lemma = WordNetLemmatizer()\n",
        "word = \"\" #@param {type:\"string\"}\n",
        "print(lemma.lemmatize(word))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fRexsQWZJCO"
      },
      "source": [
        "###**Discussion Exercise**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lfFIoeOJ-mS"
      },
      "source": [
        " What are the differences between the Porter stemmer and the lematizer? How do you think the lemmatizer works?\n",
        "\n",
        " Discuss your answer then write it down."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRcZX84-x06q"
      },
      "source": [
        "## Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IMUAPkRbasd"
      },
      "source": [
        "###**Discussion Exercise**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IdSIVSLyWMr"
      },
      "source": [
        "Are there words that can be removed without affecting the model?\n",
        "\n",
        "Write examples of a few words that you think can be removed from the sentence, but yet the sentence would not be mis-classified (Think of words that occur most common, and in both the tweet categories...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yF6R42-zoUp"
      },
      "source": [
        "Stop words are words that occur in both category, that are not relevant to the context, such as 'at', 'is', 'the' and so on... It is usually advantageous for the classifier to ignore these stop words, since they may add noises or cause numerical issues as they add baggage to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIsSdBJ14Zn_"
      },
      "source": [
        "#@title Few Stop-Words { vertical-output: true, display-mode: \"form\" }\n",
        "eng_stopwords = set(stopwords.words('english'))\n",
        "for i,word in enumerate(eng_stopwords):\n",
        "    if i>10: break\n",
        "    print(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBPkmmzF4mWU"
      },
      "source": [
        "Let us see if the words you identified are stop words or not. Check your words here, using this interactive piece of code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw75nnP14t-k"
      },
      "source": [
        "#@title Check stop words { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "word = \"\" #@param {type:\"string\"}\n",
        "if not word: raise Exception('Please enter a word')\n",
        "eng_stopwords = set(stopwords.words('english'))\n",
        "if word.lower().strip() in eng_stopwords: print('YES')\n",
        "else: print('NO')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fK0FSbhRA2r"
      },
      "source": [
        "## Preprocessing pipeline of our data\n",
        "\n",
        "Explore how combining these methods changes the structure of our tweet dataset. Here are the first 5 tweets after preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULJG6XbIpxsZ"
      },
      "source": [
        "stopword_set = set(stopwords.words('english'))\n",
        "\n",
        "'''\n",
        "Complete the following function to remove the stopwords from the tokenized tweets\n",
        "'''\n",
        "def remove_stopwords(token_list):\n",
        "  filtered_sentences = []\n",
        "  \"\"\"\n",
        "  YOUR CODE HERE\n",
        "  \"\"\"\n",
        "  return filtered_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho4sPpFJRFy2"
      },
      "source": [
        "#Tokenize all the tweets\n",
        "tokenized_tweets = [word_tokenize(t) for t in tweets]\n",
        "\n",
        "#Remove Stopwords from all the tweets\n",
        "tweet_set = remove_stopwords(tokenized_tweets)\n",
        "\n",
        "## First 5 tweets:\n",
        "for i in range(5):\n",
        "  print(\"Original tweet: %s: \\nCleaned and tokenized data: %s\\n\" % (tweets[i], tweet_set[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYmIUGamOuaN"
      },
      "source": [
        "\n",
        "# Bag of Words Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_fBuMp_UsO8"
      },
      "source": [
        "## One-Hot Encoding\n",
        "**One Hot Encoding**, also known as one-of-K scheme is a way to encode the data to be used in other functions (such as linear regression).\n",
        "\n",
        "Let us consider an example to understand one hot encoding:\n",
        "\n",
        "Before we apply a model on our tweets, we need to convert it to a form the model, i.e. a machine, can understand - esentially convert a tweet to numerical form. We cannot just pass words to the model, because it won't know what those mean and migt try and exrtract information from them. Hence, a numerical format is the best.\n",
        "\n",
        "The easiet way to do so is to map each word in a tweet to a number, a categorical value. This will represent all words in a tweet uniquely!\n",
        "\n",
        "---\n",
        "Suppose we have a tweet:\n",
        "> Tweet: 'I am hungry need food' <br>\n",
        "> Category: Food\n",
        "\n",
        "Its numerical representation would be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqSH4SwDUrol"
      },
      "source": [
        "#@title Numeric Represention { vertical-output: true, display-mode: \"form\" }\n",
        "d = {'I': 1, 'am': 2, 'hungry': 3, 'need': 4, 'food': 5}\n",
        "print('{:<12}|{:>2}'.format('word', 'value'))\n",
        "print('-------------------')\n",
        "for k,v in d.items(): print('{:<12}|{:>3}'.format(k,v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b-n3laDc1ZS"
      },
      "source": [
        "**Discussion Exercise**: Why do you think the above representation is wrong? What information could a model possibly extract from the above information such that its conclusions would be wrong/way off to what we want to achieve.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba6M5gJ9dScQ"
      },
      "source": [
        "We need to encode the words and include them as a feature to train the model. That is where One Hot Encoding comes into play. Understanding how it is done will make the process clearer\n",
        "\n",
        "Let us consider the same example and see what its one hot encoding would be"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzqRYQVfKYwZ"
      },
      "source": [
        "#@title One Hot Encoding { vertical-output: true, display-mode: \"form\" }\n",
        "print('{:^10}|{:^7}|{:^8}|{:^2}|{:^9}'.format('I', 'am', 'hungry','need','food'))\n",
        "print('---------------------------------------------------')\n",
        "print('{:^10}|{:^7}|{:^8}|{:^2}|{:^9}'.format('1', '0', '0','0','0'))\n",
        "print('{:^10}|{:^7}|{:^8}|{:^2}|{:^9}'.format('0', '1', '0','0','0'))\n",
        "print('{:^10}|{:^7}|{:^8}|{:^2}|{:^9}'.format('0', '0', '1','0','0'))\n",
        "print('{:^10}|{:^7}|{:^8}|{:^2}|{:^9}'.format('0', '0', '0','1','0'))\n",
        "print('{:^10}|{:^7}|{:^8}|{:^2}|{:^9}'.format('0', '0', '0','0','1'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4ihznS8vA3p"
      },
      "source": [
        "You see here, each letter is represented using a row of 1's and 0's, a row which can esentially represent the whole of the vocabulary. This is called one hot encoding.\n",
        "\n",
        "So 'I' would be [1,0,0,0,0]\n",
        "\n",
        "And a whole sentence is the combination of all the words and hence their one hot encoding. So the representation of the sentence 'I hungry' would be [1,0,1,0,0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqy3nTWDVYrt"
      },
      "source": [
        "### One Hot Encoding Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoMBPD60VYMd",
        "cellView": "both"
      },
      "source": [
        "def one_hot_encoding(sentences, sentence, print_word_dict = False):\n",
        "  \"\"\"\n",
        "  param: sentences - list of sentences to form the one hot encoding\n",
        "  param: sentence - sentence to return the one hot encoding of\n",
        "  return: sent_encoding - the encoded sentence\n",
        "  \"\"\"\n",
        "\n",
        "  # words_list = an empty list\n",
        "  words_list = []\n",
        "\n",
        "  # Loop through all the sentences.\n",
        "  # Split each sentence using the .split() method for a string, to get a list of words\n",
        "  # Add those words to words_list\n",
        "  for sent in sentences:\n",
        "    words_list.extend(word_tokenize(sent))\n",
        "\n",
        "  # remove the duplicates by making it into a set and back to a list\n",
        "  words_list = list(set(words_list))\n",
        "\n",
        "  # words_map_dict = make an empty dictionary\n",
        "  words_map_dict = {}\n",
        "\n",
        "  # loop through all the words in words_list\n",
        "  # add each word as a key in words_map_dict and the index of the word as value.\n",
        "  for w in range(len(words_list)):\n",
        "    word = words_list[w]\n",
        "    words_map_dict[word] = w\n",
        "\n",
        "  # sent_encoding = a numpy array of all zeros of the same length as words_list\n",
        "  sent_encoding = np.zeros(len(words_list))\n",
        "\n",
        "  # loop through the words in the given sentence (sentence to check)\n",
        "  # Find the index of each word, using the words_map_dict dictionary\n",
        "  # Change the value of the numpy array at that index to one\n",
        "  for word in word_tokenize(sentence):\n",
        "    if word not in words_map_dict:\n",
        "      continue\n",
        "    print(word)\n",
        "    indx = words_map_dict[word]\n",
        "    sent_encoding[indx] = 1\n",
        "\n",
        "  if print_word_dict:\n",
        "    print (\"Word Dictionary: \", words_map_dict)\n",
        "\n",
        "  return sent_encoding\n",
        "\n",
        "  ### Your code ends here ###\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3rJZr_hqsUX"
      },
      "source": [
        "encoding = one_hot_encoding([\"oh I love AI\", \"AI is so much fun\", \"is AI fun or what\"], \"oh what fun AI is \", True)\n",
        "print(\"Encoding: \", encoding)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGIjELh0wDD1"
      },
      "source": [
        "## Count Vectorizer - The Bag of words model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uZDNaLHdxbl"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi7A3SuRwGdP"
      },
      "source": [
        "Now that we have the one hot encoding for all tweets, its time to attach value to each word, such that it can be distinguished and used by the model.\n",
        "\n",
        "One idea is to assign each word the same weight, say a weight of one. However, that is non distinguishable and the model won't be able to learn anything with that information. Can you think of a simple way to add weights to words which, say, occur more frequently?\n",
        "\n",
        "That is the Bag of Words Model. The **Bag of Words Model** converts the tweets into a matrix of token counts. Let us consider an example\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebbwyut2jxgl"
      },
      "source": [
        "Recall the first 3 tweets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bznJPPrHeAA5"
      },
      "source": [
        "for t in tweets[:3]:\n",
        "  print(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_nJ-Kqtkvv6"
      },
      "source": [
        "The Vocabulary for the tweets would be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax8Y4gQCsibD"
      },
      "source": [
        "#@title Vocabulary { vertical-output: true, display-mode: \"form\" }\n",
        "word_count = Counter()\n",
        "for tweet in tweets[:3]:\n",
        "  for t in word_tokenize(tweet):\n",
        "    word_count[t]+=1\n",
        "word_count_list = [(k,v) for k,v in word_count.items()]\n",
        "word_count_list.sort(key=lambda x:x[0])\n",
        "print('{:<12}|{:>2}'.format('word', 'position'))\n",
        "print('-------------------')\n",
        "for k,v in enumerate(word_count_list): print('{:<12}|{:>3}'.format(v[0],k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkY5OPdZ1XXR"
      },
      "source": [
        "If there are N words in the vocabulary, each row in the matrix would be of N words.\n",
        "\n",
        "Based on this, one hot encoding for each tweet and the matrix that will be given to the model would be the following.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHKB2Kt3egP6"
      },
      "source": [
        "print(\"Tweet 1: \", one_hot_encoding([str(t) for t in tweets[:3]], str(tweets[0])))\n",
        "print(\"Tweet 2: \", one_hot_encoding([str(t) for t in tweets[:3]], str(tweets[1])))\n",
        "print(\"Tweet 3: \", one_hot_encoding([str(t) for t in tweets[:3]], str(tweets[2])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwwUjET5z_NY",
        "cellView": "form"
      },
      "source": [
        "#@title Token Counts { vertical-output: true }\n",
        "print('{:<12}|{:>2}'.format('word', 'word_count'))\n",
        "print('-------------------')\n",
        "for k,v in word_count_list: print('{:<12}|{:>3}'.format(k,v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leEZ2wKv4Iie"
      },
      "source": [
        "The above is Bag of Words model. Sklearn's Countvectorizer does the same thing, however, in a much more sophisticated manner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-TMn3LZ4g5N"
      },
      "source": [
        "train_text = [t for t in tweets[:3]]\n",
        "print(train_text)\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_text)\n",
        "print('Number of Words in Vocabulary of train tweets are: {}'.format(len(vectorizer.vocabulary_)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0J4tRe1UrL3"
      },
      "source": [
        "And the Vocabulary is *almost* the same as ours above.\n",
        "\n",
        "**Discussion Exercise**: What is different about vocabulary results from the CountVectorizer than our method above?   Why do you think that is? Which is better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzb-T1z6UnZI"
      },
      "source": [
        "vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg5gkx3jI9CG"
      },
      "source": [
        "### CountVectorizer - Fit and Transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HcyuzbvJCN6"
      },
      "source": [
        "CountVectorizer's Fit method fits the given data to the vectorizer - consider it as if it is learning from the data. It produces a matrix, which is then be passed on to Logistic regression (later, so don't worry about understanding that). Think of the fit method this way: it just allows you to generate the matrix (but doesnt let us see it) used by the model to learn information about the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijxeGiNcJnRN"
      },
      "source": [
        "CountVectorizer's Transform method transforms the given data to the matrix, there is no learning here. It does not generate a vocabulary, nor does it allow for many other functionalities that the fit method allows for. Hence, it cannot be used as a substitue for the fit method as it just spews out the matrix representation of the data!<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lawl8vijIMIs"
      },
      "source": [
        "### Coding Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRlc2lohePfX"
      },
      "source": [
        "Let us make the countvectorizer and use its fit and transform method, to learn their working. An example has been shown for you below. Try it on tweets of your choice!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "895LTm1pKjpF"
      },
      "source": [
        "tweet_01 = 'This is a big tweet '\n",
        "tweet_02 = 'Sample Tweet two'\n",
        "# You can add more tweets here\n",
        "train_text = [tweet_01, tweet_02]\n",
        "print(train_text)\n",
        "\n",
        "### Your code starts here ###\n",
        "\n",
        "# make a CountVectorizer\n",
        "# fit the data train_text to the vectorizer using the .fit method\n",
        "# print the vocabulary of the vectorizer\n",
        "\n",
        "tweet_03 = 'Sample tweet'\n",
        "# Now, transform this tweet's tokenList\n",
        "\n",
        "### Your code ends here ###\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahjK7AgNZUx6"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS_FxE83RBuU"
      },
      "source": [
        "## Logistic Regression Refresher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAd_hJNd0Ul5"
      },
      "source": [
        "We've just spent the last week or so learning about more sophisticated neural network architectures.  Remember that logistic regression is just linear regression followed by a sigmoid function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zmV-2E-VCdL"
      },
      "source": [
        "**Review:** What is Logistic Regression?\n",
        "\n",
        "Logistic regression is a type of linear regression that is generally used for classification. Unlike linear regression which outputs continuous number values, logistic regression uses the logsitic function, also called the sigmoid function, to transform the output to return a probability value between 1 and 0, which can then be mapped to the different categories. The logistic (sigmoid) function looks something like this:\n",
        "\n",
        "![Logistic Function](https://ml-cheatsheet.readthedocs.io/en/latest/_images/sigmoid.png)\n",
        "\n",
        "Consider an example to understand logistic regression and to enchance the difference between logisitc and linear regression:\n",
        "\n",
        "> Given data on time spent studying and exam scores. Linear Regression and logistic regression can predict different things:\n",
        "\n",
        ">> *Linear Regression* could help us predict the student’s test score on a scale of 0 - 100. Linear regression predictions are continuous (numbers in a range).<br>\n",
        "*Logistic Regression* could help use predict whether the student passed or failed. Logistic regression predictions are discrete (only specific values or categories are allowed). We can also view probability scores underlying the model’s classifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlicWtTrRHPC"
      },
      "source": [
        " **Why Logistic Regression?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue2m3durVBE2"
      },
      "source": [
        "**A couple of reasons for using Logistic regression:**\n",
        "1.   Using a simpler model tells us how much room we have to improve.\n",
        "2.   A simple model makes iteration quick and easy.\n",
        "3.   Lastly, and perhaps most importantly, logistic regression is interpretable. You may have heard in the past that one thing deep neural networks struggle with is interpretability–when you are using these models to make predictions that affect people's wellbeing (e.g., sentencing decisions, predictive policing decisions), it becomes extremely important that you are able to understand why a model is making the predictions it makes.For simpler models like logistic regression, we get interpretability for free!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwJUL4QDQ-KB"
      },
      "source": [
        "## Logistic Regression in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NluqLKKQ8SNv"
      },
      "source": [
        "Logistic regression in python can be done easily with the help of sklearn's Logistic Regression function. Let us first do it for the three tweet examples that we saw above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0xxnn1i8KoK"
      },
      "source": [
        "tweet1 = 'please help we desperately need food'\n",
        "label1 = \"Food\"\n",
        "tweet2 = 'We are very thirsty please send water'\n",
        "label2 = 'Water'\n",
        "tweet3 = 'we need water and are very thirsty'\n",
        "label3 = 'Water'\n",
        "\n",
        "train_tweets = [tweet1, tweet2]\n",
        "train_tweets_label = [label1, label2]\n",
        "test_tweets = [tweet3]\n",
        "test_tweets_label = [label3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVEdW2d8RO2i"
      },
      "source": [
        "Next, let us make the vectorizer and encode tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aHfIV639W-o"
      },
      "source": [
        "vectorizer = CountVectorizer() # Countvectorizer\n",
        "train_vect = vectorizer.fit_transform(train_tweets) # fit_transform fits the train tweets and returns the sparse matrix of the tweets\n",
        "model = LogisticRegression() # create a LogisticRegression Model\n",
        "model.fit(train_vect, train_tweets_label) # Fit the data values to the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAzZHe6F-iOI"
      },
      "source": [
        "Over here, model.fit(train_vect, train_tweets_label) applies logistic regression to the data given by the  matrix and hence, fits the data to the function. It takes two arguments: the matrix that is the train_vect variable and the train_tweets_label, which is the category each tweet belongs to.\n",
        "\n",
        "Let us now predict the third tweet using this model, using the method predict. However, first we need to transform the tweet into a vector form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8hxXm66--ox"
      },
      "source": [
        "test_vect = vectorizer.transform(test_tweets)\n",
        "result = model.predict(test_vect)\n",
        "print('Actual Category: {}\\nPredicted Category: {}'.format(label3, result[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__6MSOHNUy_2"
      },
      "source": [
        "Yay! It predicted it correctly! However, that might not always be the case, as the training set here is so small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9eYK4QDT9Pg"
      },
      "source": [
        "**Exercise**: Can you trick the logistic regression? Try and make a tweet get classified as \"Water\" when it is about \"Food\".  *Hint: What are some words that were in the Food tweet in the training set that had no significance to food?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJnVDK-oUAQV"
      },
      "source": [
        "#@title Trick the logistic regression { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "test_tweet = \"Apple juice is the best\" #@param {type:\"string\"}\n",
        "true_label = \"Food\" #@param [\"Food\", \"Water\"]\n",
        "\n",
        "test_vect = vectorizer.transform([test_tweet])\n",
        "result = model.predict(test_vect)\n",
        "print('Actual Category: {}\\nPredicted Category: {}'.format(true_label, result[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDaw9DovKNpK"
      },
      "source": [
        "## Logistic Regression for Tweet Classification (Coding Exercise)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Fs9EU6FSvdF"
      },
      "source": [
        "#Split the Data into Training and Testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweet_set, tweet_labels, test_size=0.2, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amJz-5wbKP1J"
      },
      "source": [
        "Let us now build our own regression model for all the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z65-9oJQ7mpZ"
      },
      "source": [
        "def train_model(tweets_to_train,train_labels):\n",
        "  \"\"\"\n",
        "  param: tweets_to_train - list of tweets to train on\n",
        "  return: the vectorizer, the logistic regression model, the train_vector\n",
        "  \"\"\"\n",
        "\n",
        "  train_tweets = [\"\".join(t) for t in tweets_to_train]\n",
        "  train_tweets_label = [l for l in train_labels]\n",
        "\n",
        "\n",
        "  ### Your code starts here ###\n",
        "\n",
        "  # vectorizer = Initialize CountVectorizer\n",
        "  # fit the train_tweets in the CountVectorizer using the vectorizer's fit method\n",
        "  # train_vect = get the sparse matrix of the train_tweets from the vectorizer using the transform function\n",
        "\n",
        "  # model = initialize a Logistic Regression model\n",
        "  # fit train_vect and train_tweets_label using the fit function of LogisticRegression\n",
        "\n",
        "  ### Your code ends here ###\n",
        "\n",
        "  #train_tweets = [str(t) for t in tweets_to_train]\n",
        "  #train_tweets_label = [t.category for t in tweets_to_train]\n",
        "\n",
        "  vectorizer = ___\n",
        "  train_vect = vectorizer.fit_transform(train_tweets)\n",
        "\n",
        "  model = ___ # create a LogisticRegression Model\n",
        "  model.fit(___, ___)\n",
        "  model = LogisticRegression() # create a LogisticRegression Model\n",
        "  model.fit(train_vect, train_tweets_label)\n",
        "\n",
        "\n",
        "  return model,train_vect\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9s_a32jxXXQ"
      },
      "source": [
        "def predict(tweets_to_test, vectorizer, model):\n",
        "  \"\"\"\n",
        "  param: tweets_to_test - list of tweets to test the model on\n",
        "  param: vectorizer - the CountVectorizer\n",
        "  param: model - the LogisticRegression model\n",
        "  return result (the prediction), the test_vect\n",
        "  \"\"\"\n",
        "\n",
        "  test_tweets = [\" \".join(t) for t in  tweets_to_test]\n",
        "\n",
        "  print (test_tweets)\n",
        "  ### Your code starts here ###\n",
        "\n",
        "  # test_vect = transform the test_tweets to sparce matrix using the vectorizer's transform function\n",
        "  test_vect = ____\n",
        "  # result = predict the result using the model's predict function on test_vect\n",
        "  result = ____\n",
        "\n",
        "  return result\n",
        "\n",
        "  ### Your code ends here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeSN-6b1STF7"
      },
      "source": [
        "model,train_countvect = train_model(X_train,y_train)\n",
        "#Predict labels for test set\n",
        "y_pred = predict (X_test,train_countvect,model)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOWekFQkXPjP"
      },
      "source": [
        "# Evaluation\n",
        "Let's see how our classifier did!  We will train our classifier on 80% of the dataset and then test it on 20%. This is called a *train-test split* and is usually done to evaluate models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XadKkUt39krv"
      },
      "source": [
        "table=pd.DataFrame([[\" \".join(t) for t in X_test],y_pred, y_test]).transpose()\n",
        "table.columns = ['Tweet', 'Predicted Category', 'True Category']\n",
        "print(\"Percent Correct: %.2f\" % (sum(table['Predicted Category'] == table['True Category'])/len(table['True Category'])))\n",
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaPh1b8-ZCuk"
      },
      "source": [
        "**Discussion Exercise** Which categories does the regressor perform best on?  Would the classifier perform better or worse if we only used the food vs water tweets?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvwjKTj9j58o"
      },
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "Let us look at some stats about the prediction to understand what the model predicted! Review day 1 for a refresher on accuracy-related metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AWBAFspnklt",
        "cellView": "form"
      },
      "source": [
        "#@title Helper Function-Confusion Matrix\n",
        "'''\n",
        "Plots the confusion Matrix and saves it\n",
        "'''\n",
        "def plot_confusion_matrix(y_true,y_predicted):\n",
        "  cm = metrics.confusion_matrix(y_true, y_predicted)\n",
        "  print (\"Plotting the Confusion Matrix\")\n",
        "  labels = ['Energy', 'Food', 'Medical', 'None', 'Water']\n",
        "  df_cm = pd.DataFrame(cm,index =labels,columns = labels)\n",
        "  fig = plt.figure()\n",
        "  res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "  plt.yticks([0.5,1.5,2.5,3.5,4.5], labels,va='center')\n",
        "  plt.title('Confusion Matrix - TestData')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAqXjDIcBvnq"
      },
      "source": [
        "plot_confusion_matrix(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjNNguK_-esv"
      },
      "source": [
        "Review day 1 for a refresher on accuracy metrics!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRLVrhvdB7Xj"
      },
      "source": [
        "print('The total number of correct predictions are: {}'.format(sum(table['Predicted Category'] == table['True Category'])))\n",
        "print('The total number of incorrect predictions are: {}'.format(sum(table['Predicted Category'] != table['True Category'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW0GoJB1_wbK"
      },
      "source": [
        "print('Accuracy on the test data is: {:.2f}%'.format(metrics.accuracy_score(y_test, y_pred)*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcmf1R50XvLC"
      },
      "source": [
        "###Exercise: Discussion\n",
        "\n",
        "Comment out the following code line in the Data Preprocessing part and run through the algorithm again!\n",
        "\n",
        "' tweets = tweets.apply(lambda x: re.sub(r'[^a-zA-Z0-9]+', ' ',x))''\n",
        "\n",
        "Comment on the classifier's accuracy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gg1B7NfkfzA"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1m0JuOtkhmi"
      },
      "source": [
        "**Discussion Exercise**:\n",
        "1. How could you use what you have built to help during a disaster?\n",
        "\n",
        "\n",
        "That is it for today! Tomorrow we shall cover another model - GloVe. Review the concepts we covered today and enjoy the rest of your day!\n"
      ]
    }
  ]
}